{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "772626f1",
   "metadata": {},
   "source": [
    "## Name : Trupti Kanade\n",
    "## Contact Number : +91 9552828899\n",
    "## Email ID : truptiproject07@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63b688a",
   "metadata": {},
   "source": [
    "# Explaination:Install Important Libraries in command prompt\n",
    "### pip install pandas\n",
    "### pip install requests\n",
    "### pip install beautifulsoup4\n",
    "### pip install nltk\n",
    "### pip install textblob\n",
    "### pip install python-docx\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeab7b28",
   "metadata": {},
   "source": [
    "# How to run this code..?\n",
    "#### -->  Just copy the code and paste in VS code / Sublimetext. Add the given excel data set and docx file in same python folder 1)Text Analysis.Docx , 2)Input.xlsx , 3) Output Data Structure.xlsx.\n",
    "#### If you use sublime text then save and press Ctrl+B and run the code.\n",
    "#### If in case you don't understand ant thing in this so simply open the given 1356.ipynb file in  this file output present after every line for opening this file use anaconda navigator.\n",
    "#### Final Output present in 1)text_analysis_results.xlsx excel sheet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2841eed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\trupti kanade\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: requests in c:\\users\\trupti kanade\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\trupti kanade\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\trupti kanade\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\trupti kanade\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\trupti kanade\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\trupti kanade\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\trupti kanade\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\trupti kanade\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\trupti kanade\\anaconda3\\lib\\site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\trupti kanade\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\trupti kanade\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\trupti kanade\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53639162",
   "metadata": {},
   "source": [
    "# Explaination-->Data Extraction\n",
    "###  1)Used the pandas library to read the input Excel file containing URLs.\n",
    "### 2) Utilized the BeautifulSoup library to extract text from each URL.\n",
    "### 3) Saved the extracted text in a new Excel file.(extracted_text.xlsx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3cd2d4",
   "metadata": {},
   "source": [
    "# Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dc2fef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2c966ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input Excel file\n",
    "input_file = \"Input.xlsx\"\n",
    "df = pd.read_excel(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88b2484f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>blackassign0096</td>\n",
       "      <td>https://insights.blackcoffer.com/what-is-the-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>blackassign0097</td>\n",
       "      <td>https://insights.blackcoffer.com/impact-of-cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>blackassign0098</td>\n",
       "      <td>https://insights.blackcoffer.com/contribution-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>blackassign0099</td>\n",
       "      <td>https://insights.blackcoffer.com/how-covid-19-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>blackassign0100</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-covi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             URL_ID                                                URL\n",
       "0   blackassign0001  https://insights.blackcoffer.com/rising-it-cit...\n",
       "1   blackassign0002  https://insights.blackcoffer.com/rising-it-cit...\n",
       "2   blackassign0003  https://insights.blackcoffer.com/internet-dema...\n",
       "3   blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...\n",
       "4   blackassign0005  https://insights.blackcoffer.com/ott-platform-...\n",
       "..              ...                                                ...\n",
       "95  blackassign0096  https://insights.blackcoffer.com/what-is-the-r...\n",
       "96  blackassign0097  https://insights.blackcoffer.com/impact-of-cov...\n",
       "97  blackassign0098  https://insights.blackcoffer.com/contribution-...\n",
       "98  blackassign0099  https://insights.blackcoffer.com/how-covid-19-...\n",
       "99  blackassign0100  https://insights.blackcoffer.com/how-will-covi...\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c10f65b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from URL\n",
    "def extract_text_from_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Extract text from HTML\n",
    "        text = soup.get_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b154b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column for extracted text\n",
    "df['Extracted Text'] = df['URL'].apply(extract_text_from_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c4515e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Extracted Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nRising IT cities and its impact on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nRising IT Cities and Their Impact ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nInternet Demand's Evolution, Commu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nRise of Cybercrime and its Effect ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nOTT platform and its impact on the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>blackassign0096</td>\n",
       "      <td>https://insights.blackcoffer.com/what-is-the-r...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nDue to the COVID-19 the repercussi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>blackassign0097</td>\n",
       "      <td>https://insights.blackcoffer.com/impact-of-cov...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nImpact of COVID-19 pandemic on off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>blackassign0098</td>\n",
       "      <td>https://insights.blackcoffer.com/contribution-...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nContribution of handicrafts (Visua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>blackassign0099</td>\n",
       "      <td>https://insights.blackcoffer.com/how-covid-19-...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nHow COVID-19 is impacting payment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>blackassign0100</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-covi...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nHow will COVID-19 affect the world...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             URL_ID                                                URL  \\\n",
       "0   blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "1   blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "2   blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "3   blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "4   blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
       "..              ...                                                ...   \n",
       "95  blackassign0096  https://insights.blackcoffer.com/what-is-the-r...   \n",
       "96  blackassign0097  https://insights.blackcoffer.com/impact-of-cov...   \n",
       "97  blackassign0098  https://insights.blackcoffer.com/contribution-...   \n",
       "98  blackassign0099  https://insights.blackcoffer.com/how-covid-19-...   \n",
       "99  blackassign0100  https://insights.blackcoffer.com/how-will-covi...   \n",
       "\n",
       "                                       Extracted Text  \n",
       "0   \\n\\n\\n  \\n\\nRising IT cities and its impact on...  \n",
       "1   \\n\\n\\n  \\n\\nRising IT Cities and Their Impact ...  \n",
       "2   \\n\\n\\n  \\n\\nInternet Demand's Evolution, Commu...  \n",
       "3   \\n\\n\\n  \\n\\nRise of Cybercrime and its Effect ...  \n",
       "4   \\n\\n\\n  \\n\\nOTT platform and its impact on the...  \n",
       "..                                                ...  \n",
       "95  \\n\\n\\n  \\n\\nDue to the COVID-19 the repercussi...  \n",
       "96  \\n\\n\\n  \\n\\nImpact of COVID-19 pandemic on off...  \n",
       "97  \\n\\n\\n  \\n\\nContribution of handicrafts (Visua...  \n",
       "98  \\n\\n\\n  \\n\\nHow COVID-19 is impacting payment ...  \n",
       "99  \\n\\n\\n  \\n\\nHow will COVID-19 affect the world...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "119d2b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated DataFrame\n",
    "df.to_excel('extracted_text.xlsx', index=False)  # Change 'extracted_text.xlsx' to your desired output file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cb0be14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Extracted Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nRising IT cities and its impact on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nRising IT Cities and Their Impact ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nInternet Demand's Evolution, Commu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nRise of Cybercrime and its Effect ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nOTT platform and its impact on the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>blackassign0096</td>\n",
       "      <td>https://insights.blackcoffer.com/what-is-the-r...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nDue to the COVID-19 the repercussi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>blackassign0097</td>\n",
       "      <td>https://insights.blackcoffer.com/impact-of-cov...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nImpact of COVID-19 pandemic on off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>blackassign0098</td>\n",
       "      <td>https://insights.blackcoffer.com/contribution-...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nContribution of handicrafts (Visua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>blackassign0099</td>\n",
       "      <td>https://insights.blackcoffer.com/how-covid-19-...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nHow COVID-19 is impacting payment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>blackassign0100</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-covi...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nHow will COVID-19 affect the world...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             URL_ID                                                URL  \\\n",
       "0   blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "1   blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "2   blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "3   blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "4   blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
       "..              ...                                                ...   \n",
       "95  blackassign0096  https://insights.blackcoffer.com/what-is-the-r...   \n",
       "96  blackassign0097  https://insights.blackcoffer.com/impact-of-cov...   \n",
       "97  blackassign0098  https://insights.blackcoffer.com/contribution-...   \n",
       "98  blackassign0099  https://insights.blackcoffer.com/how-covid-19-...   \n",
       "99  blackassign0100  https://insights.blackcoffer.com/how-will-covi...   \n",
       "\n",
       "                                       Extracted Text  \n",
       "0   \\n\\n\\n  \\n\\nRising IT cities and its impact on...  \n",
       "1   \\n\\n\\n  \\n\\nRising IT Cities and Their Impact ...  \n",
       "2   \\n\\n\\n  \\n\\nInternet Demand's Evolution, Commu...  \n",
       "3   \\n\\n\\n  \\n\\nRise of Cybercrime and its Effect ...  \n",
       "4   \\n\\n\\n  \\n\\nOTT platform and its impact on the...  \n",
       "..                                                ...  \n",
       "95  \\n\\n\\n  \\n\\nDue to the COVID-19 the repercussi...  \n",
       "96  \\n\\n\\n  \\n\\nImpact of COVID-19 pandemic on off...  \n",
       "97  \\n\\n\\n  \\n\\nContribution of handicrafts (Visua...  \n",
       "98  \\n\\n\\n  \\n\\nHow COVID-19 is impacting payment ...  \n",
       "99  \\n\\n\\n  \\n\\nHow will COVID-19 affect the world...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357d45f3",
   "metadata": {},
   "source": [
    "# Update name check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a04d908b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Extracted Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nRising IT cities and its impact on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nRising IT Cities and Their Impact ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nInternet Demand's Evolution, Commu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nRise of Cybercrime and its Effect ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nOTT platform and its impact on the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>blackassign0096</td>\n",
       "      <td>https://insights.blackcoffer.com/what-is-the-r...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nDue to the COVID-19 the repercussi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>blackassign0097</td>\n",
       "      <td>https://insights.blackcoffer.com/impact-of-cov...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nImpact of COVID-19 pandemic on off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>blackassign0098</td>\n",
       "      <td>https://insights.blackcoffer.com/contribution-...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nContribution of handicrafts (Visua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>blackassign0099</td>\n",
       "      <td>https://insights.blackcoffer.com/how-covid-19-...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nHow COVID-19 is impacting payment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>blackassign0100</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-covi...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nHow will COVID-19 affect the world...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             URL_ID                                                URL  \\\n",
       "0   blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "1   blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "2   blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "3   blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "4   blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
       "..              ...                                                ...   \n",
       "95  blackassign0096  https://insights.blackcoffer.com/what-is-the-r...   \n",
       "96  blackassign0097  https://insights.blackcoffer.com/impact-of-cov...   \n",
       "97  blackassign0098  https://insights.blackcoffer.com/contribution-...   \n",
       "98  blackassign0099  https://insights.blackcoffer.com/how-covid-19-...   \n",
       "99  blackassign0100  https://insights.blackcoffer.com/how-will-covi...   \n",
       "\n",
       "                                       Extracted Text  \n",
       "0   \\n\\n\\n  \\n\\nRising IT cities and its impact on...  \n",
       "1   \\n\\n\\n  \\n\\nRising IT Cities and Their Impact ...  \n",
       "2   \\n\\n\\n  \\n\\nInternet Demand's Evolution, Commu...  \n",
       "3   \\n\\n\\n  \\n\\nRise of Cybercrime and its Effect ...  \n",
       "4   \\n\\n\\n  \\n\\nOTT platform and its impact on the...  \n",
       "..                                                ...  \n",
       "95  \\n\\n\\n  \\n\\nDue to the COVID-19 the repercussi...  \n",
       "96  \\n\\n\\n  \\n\\nImpact of COVID-19 pandemic on off...  \n",
       "97  \\n\\n\\n  \\n\\nContribution of handicrafts (Visua...  \n",
       "98  \\n\\n\\n  \\n\\nHow COVID-19 is impacting payment ...  \n",
       "99  \\n\\n\\n  \\n\\nHow will COVID-19 affect the world...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file_1 = \"extracted_text.xlsx\"\n",
    "df = pd.read_excel(input_file_1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42858d0",
   "metadata": {},
   "source": [
    "# Explanination-->Data Analysis\n",
    "### 1) Performed data analysis on the extracted text to compute variables like word count, unique word count, average word length, and lexical diversity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665740dd",
   "metadata": {},
   "source": [
    "# Step 2 Data Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0a99915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction completed. Text files saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# Load the Excel sheet\n",
    "df = pd.read_excel('Input.xlsx') \n",
    "\n",
    "\n",
    "# Function to extract article text from URL\n",
    "def extract_article_text(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Find article title\n",
    "        title = soup.find('title').get_text().strip()\n",
    "        # Find article text\n",
    "        article_text = ''\n",
    "        # Find all paragraphs within the main article body\n",
    "        article_paragraphs = soup.find_all('p')\n",
    "        for paragraph in article_paragraphs:\n",
    "            article_text += paragraph.get_text().strip() + '\\n'\n",
    "        return title, article_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Add new columns for article title and text\n",
    "df['Title'], df['Article Text'] = zip(*df['URL'].apply(extract_article_text))\n",
    "\n",
    "# Function to clean title for file name\n",
    "def clean_title(title):\n",
    "    # Remove special characters and spaces\n",
    "    clean_title = re.sub(r'[^\\w\\s]', '', title)\n",
    "    # Replace spaces with underscores\n",
    "    clean_title = clean_title.replace(' ', '_')\n",
    "    return clean_title\n",
    "\n",
    "# Save each article text in a separate text file\n",
    "for index, row in df.iterrows():\n",
    "    title = row['Title']\n",
    "    article_text = row['Article Text']\n",
    "    url_id = row['URL_ID']\n",
    "    if title and article_text:\n",
    "        # Clean title for file name\n",
    "        file_name = clean_title(title)\n",
    "        with open(f\"{url_id}_{file_name}.txt\", 'w', encoding='utf-8') as file:\n",
    "            file.write(f\"Title: {title}\\n\\n\")\n",
    "            file.write(f\"Article Text:\\n{article_text}\")\n",
    "\n",
    "print(\"Extraction completed. Text files saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23983997",
   "metadata": {},
   "source": [
    "# Explaination-->Data Analysis\n",
    "### 1) Used the 'NLTK' library for text processing tasks such as tokenization, stopword removal, and calculating various text statistics.\n",
    "### 2) Saved the computed variables in an Excel file.(extracted_text.xlsx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e84b2f9",
   "metadata": {},
   "source": [
    "# Step 3 Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78b1f614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\trupti kanade\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\trupti kanade\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\trupti kanade\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\trupti kanade\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\trupti kanade\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\trupti kanade\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6f6a462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "import string\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a0e52b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\trupti\n",
      "[nltk_data]     kanade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\trupti\n",
      "[nltk_data]     kanade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d122f6c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Extracted Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nRising IT cities and its impact on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nRising IT Cities and Their Impact ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nInternet Demand's Evolution, Commu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nRise of Cybercrime and its Effect ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nOTT platform and its impact on the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>blackassign0096</td>\n",
       "      <td>https://insights.blackcoffer.com/what-is-the-r...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nDue to the COVID-19 the repercussi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>blackassign0097</td>\n",
       "      <td>https://insights.blackcoffer.com/impact-of-cov...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nImpact of COVID-19 pandemic on off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>blackassign0098</td>\n",
       "      <td>https://insights.blackcoffer.com/contribution-...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nContribution of handicrafts (Visua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>blackassign0099</td>\n",
       "      <td>https://insights.blackcoffer.com/how-covid-19-...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nHow COVID-19 is impacting payment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>blackassign0100</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-covi...</td>\n",
       "      <td>\\n\\n\\n  \\n\\nHow will COVID-19 affect the world...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             URL_ID                                                URL  \\\n",
       "0   blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "1   blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "2   blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "3   blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "4   blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
       "..              ...                                                ...   \n",
       "95  blackassign0096  https://insights.blackcoffer.com/what-is-the-r...   \n",
       "96  blackassign0097  https://insights.blackcoffer.com/impact-of-cov...   \n",
       "97  blackassign0098  https://insights.blackcoffer.com/contribution-...   \n",
       "98  blackassign0099  https://insights.blackcoffer.com/how-covid-19-...   \n",
       "99  blackassign0100  https://insights.blackcoffer.com/how-will-covi...   \n",
       "\n",
       "                                       Extracted Text  \n",
       "0   \\n\\n\\n  \\n\\nRising IT cities and its impact on...  \n",
       "1   \\n\\n\\n  \\n\\nRising IT Cities and Their Impact ...  \n",
       "2   \\n\\n\\n  \\n\\nInternet Demand's Evolution, Commu...  \n",
       "3   \\n\\n\\n  \\n\\nRise of Cybercrime and its Effect ...  \n",
       "4   \\n\\n\\n  \\n\\nOTT platform and its impact on the...  \n",
       "..                                                ...  \n",
       "95  \\n\\n\\n  \\n\\nDue to the COVID-19 the repercussi...  \n",
       "96  \\n\\n\\n  \\n\\nImpact of COVID-19 pandemic on off...  \n",
       "97  \\n\\n\\n  \\n\\nContribution of handicrafts (Visua...  \n",
       "98  \\n\\n\\n  \\n\\nHow COVID-19 is impacting payment ...  \n",
       "99  \\n\\n\\n  \\n\\nHow will COVID-19 affect the world...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the extracted text data\n",
    "df = pd.read_excel('extracted_text.xlsx')  \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "755e9a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute variables from text\n",
    "def compute_variables(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Remove punctuation and stopwords\n",
    "    tokens = [word for word in tokens if word.isalnum() and word not in stopwords.words('english')]\n",
    "    # Compute word count\n",
    "    word_count = len(tokens)\n",
    "    # Compute unique word count\n",
    "    unique_words = set(tokens)\n",
    "    unique_word_count = len(unique_words)\n",
    "    # Compute average word length\n",
    "    total_word_length = sum(len(word) for word in tokens)\n",
    "    average_word_length = total_word_length / word_count if word_count > 0 else 0\n",
    "    # Compute lexical diversity\n",
    "    lexical_diversity = unique_word_count / word_count if word_count > 0 else 0\n",
    "    \n",
    "    return word_count, unique_word_count, average_word_length, lexical_diversity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b933bfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute variables for each extracted text\n",
    "variable_data = []\n",
    "for index, row in df.iterrows():\n",
    "    text = row['Extracted Text']\n",
    "    if pd.notnull(text):\n",
    "        word_count, unique_word_count, average_word_length, lexical_diversity = compute_variables(text)\n",
    "        variable_data.append([word_count, unique_word_count, average_word_length, lexical_diversity])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e94523b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Unique Word Count</th>\n",
       "      <th>Average Word Length</th>\n",
       "      <th>Lexical Diversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1309</td>\n",
       "      <td>547</td>\n",
       "      <td>7.025974</td>\n",
       "      <td>0.417876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1560</td>\n",
       "      <td>792</td>\n",
       "      <td>7.365385</td>\n",
       "      <td>0.507692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1346</td>\n",
       "      <td>618</td>\n",
       "      <td>7.835810</td>\n",
       "      <td>0.459138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1343</td>\n",
       "      <td>679</td>\n",
       "      <td>7.692480</td>\n",
       "      <td>0.505585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1092</td>\n",
       "      <td>556</td>\n",
       "      <td>7.400183</td>\n",
       "      <td>0.509158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1298</td>\n",
       "      <td>687</td>\n",
       "      <td>7.333590</td>\n",
       "      <td>0.529276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1188</td>\n",
       "      <td>582</td>\n",
       "      <td>7.133838</td>\n",
       "      <td>0.489899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>906</td>\n",
       "      <td>452</td>\n",
       "      <td>7.387417</td>\n",
       "      <td>0.498896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1041</td>\n",
       "      <td>486</td>\n",
       "      <td>7.152738</td>\n",
       "      <td>0.466859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1278</td>\n",
       "      <td>664</td>\n",
       "      <td>7.301252</td>\n",
       "      <td>0.519562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Word Count  Unique Word Count  Average Word Length  Lexical Diversity\n",
       "0         1309                547             7.025974           0.417876\n",
       "1         1560                792             7.365385           0.507692\n",
       "2         1346                618             7.835810           0.459138\n",
       "3         1343                679             7.692480           0.505585\n",
       "4         1092                556             7.400183           0.509158\n",
       "..         ...                ...                  ...                ...\n",
       "95        1298                687             7.333590           0.529276\n",
       "96        1188                582             7.133838           0.489899\n",
       "97         906                452             7.387417           0.498896\n",
       "98        1041                486             7.152738           0.466859\n",
       "99        1278                664             7.301252           0.519562\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame for computed variables\n",
    "variable_df = pd.DataFrame(variable_data, columns=['Word Count', 'Unique Word Count', 'Average Word Length', 'Lexical Diversity'])\n",
    "variable_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "668160e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the computed variables in the exact order as given in the output structure Excel file\n",
    "output_structure = pd.read_excel('Output Data Structure.xlsx')  # Load output structure\n",
    "output_structure['Word Count'] = variable_df['Word Count']\n",
    "output_structure['Unique Word Count'] = variable_df['Unique Word Count']\n",
    "output_structure['Average Word Length'] = variable_df['Average Word Length']\n",
    "output_structure['Lexical Diversity'] = variable_df['Lexical Diversity']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c004af0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the output with computed variables\n",
    "output_structure.to_excel('computed_variables_output.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "657da721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Unique Word Count</th>\n",
       "      <th>Average Word Length</th>\n",
       "      <th>Lexical Diversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1309</td>\n",
       "      <td>547</td>\n",
       "      <td>7.025974</td>\n",
       "      <td>0.417876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1560</td>\n",
       "      <td>792</td>\n",
       "      <td>7.365385</td>\n",
       "      <td>0.507692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1346</td>\n",
       "      <td>618</td>\n",
       "      <td>7.835810</td>\n",
       "      <td>0.459138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1343</td>\n",
       "      <td>679</td>\n",
       "      <td>7.692480</td>\n",
       "      <td>0.505585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1092</td>\n",
       "      <td>556</td>\n",
       "      <td>7.400183</td>\n",
       "      <td>0.509158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>blackassign0096</td>\n",
       "      <td>https://insights.blackcoffer.com/what-is-the-r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1298</td>\n",
       "      <td>687</td>\n",
       "      <td>7.333590</td>\n",
       "      <td>0.529276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>blackassign0097</td>\n",
       "      <td>https://insights.blackcoffer.com/impact-of-cov...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1188</td>\n",
       "      <td>582</td>\n",
       "      <td>7.133838</td>\n",
       "      <td>0.489899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>blackassign0098</td>\n",
       "      <td>https://insights.blackcoffer.com/contribution-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>906</td>\n",
       "      <td>452</td>\n",
       "      <td>7.387417</td>\n",
       "      <td>0.498896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>blackassign0099</td>\n",
       "      <td>https://insights.blackcoffer.com/how-covid-19-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1041</td>\n",
       "      <td>486</td>\n",
       "      <td>7.152738</td>\n",
       "      <td>0.466859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>blackassign0100</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-covi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1278</td>\n",
       "      <td>664</td>\n",
       "      <td>7.301252</td>\n",
       "      <td>0.519562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             URL_ID                                                URL  \\\n",
       "0   blackassign0001  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "1   blackassign0002  https://insights.blackcoffer.com/rising-it-cit...   \n",
       "2   blackassign0003  https://insights.blackcoffer.com/internet-dema...   \n",
       "3   blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...   \n",
       "4   blackassign0005  https://insights.blackcoffer.com/ott-platform-...   \n",
       "..              ...                                                ...   \n",
       "95  blackassign0096  https://insights.blackcoffer.com/what-is-the-r...   \n",
       "96  blackassign0097  https://insights.blackcoffer.com/impact-of-cov...   \n",
       "97  blackassign0098  https://insights.blackcoffer.com/contribution-...   \n",
       "98  blackassign0099  https://insights.blackcoffer.com/how-covid-19-...   \n",
       "99  blackassign0100  https://insights.blackcoffer.com/how-will-covi...   \n",
       "\n",
       "    POSITIVE SCORE NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
       "0              NaN            NaN             NaN                 NaN   \n",
       "1              NaN            NaN             NaN                 NaN   \n",
       "2              NaN            NaN             NaN                 NaN   \n",
       "3              NaN            NaN             NaN                 NaN   \n",
       "4              NaN                            NaN                 NaN   \n",
       "..             ...            ...             ...                 ...   \n",
       "95             NaN            NaN             NaN                 NaN   \n",
       "96             NaN            NaN             NaN                 NaN   \n",
       "97             NaN            NaN             NaN                 NaN   \n",
       "98             NaN            NaN             NaN                 NaN   \n",
       "99             NaN            NaN             NaN                 NaN   \n",
       "\n",
       "    AVG SENTENCE LENGTH  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
       "0                   NaN                          NaN        NaN   \n",
       "1                   NaN                          NaN        NaN   \n",
       "2                   NaN                          NaN        NaN   \n",
       "3                   NaN                          NaN        NaN   \n",
       "4                   NaN                          NaN        NaN   \n",
       "..                  ...                          ...        ...   \n",
       "95                  NaN                          NaN        NaN   \n",
       "96                  NaN                          NaN        NaN   \n",
       "97                  NaN                          NaN        NaN   \n",
       "98                  NaN                          NaN        NaN   \n",
       "99                  NaN                          NaN        NaN   \n",
       "\n",
       "    AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
       "0                                NaN                 NaN         NaN   \n",
       "1                                NaN                 NaN         NaN   \n",
       "2                                NaN                 NaN         NaN   \n",
       "3                                NaN                 NaN         NaN   \n",
       "4                                NaN                 NaN         NaN   \n",
       "..                               ...                 ...         ...   \n",
       "95                               NaN                 NaN         NaN   \n",
       "96                               NaN                 NaN         NaN   \n",
       "97                               NaN                 NaN         NaN   \n",
       "98                               NaN                 NaN         NaN   \n",
       "99                               NaN                 NaN         NaN   \n",
       "\n",
       "    SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  Word Count  \\\n",
       "0                 NaN                NaN              NaN        1309   \n",
       "1                 NaN                NaN              NaN        1560   \n",
       "2                 NaN                NaN              NaN        1346   \n",
       "3                 NaN                NaN              NaN        1343   \n",
       "4                 NaN                NaN              NaN        1092   \n",
       "..                ...                ...              ...         ...   \n",
       "95                NaN                NaN              NaN        1298   \n",
       "96                NaN                NaN              NaN        1188   \n",
       "97                NaN                NaN              NaN         906   \n",
       "98                NaN                NaN              NaN        1041   \n",
       "99                NaN                NaN              NaN        1278   \n",
       "\n",
       "    Unique Word Count  Average Word Length  Lexical Diversity  \n",
       "0                 547             7.025974           0.417876  \n",
       "1                 792             7.365385           0.507692  \n",
       "2                 618             7.835810           0.459138  \n",
       "3                 679             7.692480           0.505585  \n",
       "4                 556             7.400183           0.509158  \n",
       "..                ...                  ...                ...  \n",
       "95                687             7.333590           0.529276  \n",
       "96                582             7.133838           0.489899  \n",
       "97                452             7.387417           0.498896  \n",
       "98                486             7.152738           0.466859  \n",
       "99                664             7.301252           0.519562  \n",
       "\n",
       "[100 rows x 19 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('computed_variables_output.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6284c99f",
   "metadata": {},
   "source": [
    "# Explaination-->Variable\n",
    "### 1)Performed additional text analysis to calculate variables such as positive score, negative score, polarity score, subjectivity score, etc.\n",
    "### 2) Used the TextBlob library for sentiment analysis and other text analysis tasks.\n",
    "### 3) Saved the results in another Excel file.(text_analysis_results.xlsx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c3f38c",
   "metadata": {},
   "source": [
    "# Step 4 Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd1ca270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\users\\trupti kanade\\anaconda3\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\trupti kanade\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\trupti kanade\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\trupti kanade\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\trupti kanade\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\trupti kanade\\anaconda3\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n",
      "Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "   ---------------------------------------- 0.0/626.3 kB ? eta -:--:--\n",
      "   - -------------------------------------- 30.7/626.3 kB 1.3 MB/s eta 0:00:01\n",
      "   - -------------------------------------- 30.7/626.3 kB 1.3 MB/s eta 0:00:01\n",
      "   -- ------------------------------------ 41.0/626.3 kB 326.8 kB/s eta 0:00:02\n",
      "   -- ------------------------------------ 41.0/626.3 kB 326.8 kB/s eta 0:00:02\n",
      "   ----- --------------------------------- 81.9/626.3 kB 327.3 kB/s eta 0:00:02\n",
      "   ----- --------------------------------- 81.9/626.3 kB 327.3 kB/s eta 0:00:02\n",
      "   ----- --------------------------------- 81.9/626.3 kB 327.3 kB/s eta 0:00:02\n",
      "   ----- --------------------------------- 81.9/626.3 kB 327.3 kB/s eta 0:00:02\n",
      "   ------ ------------------------------- 112.6/626.3 kB 251.6 kB/s eta 0:00:03\n",
      "   -------- ----------------------------- 143.4/626.3 kB 293.9 kB/s eta 0:00:02\n",
      "   -------- ----------------------------- 143.4/626.3 kB 293.9 kB/s eta 0:00:02\n",
      "   -------- ----------------------------- 143.4/626.3 kB 293.9 kB/s eta 0:00:02\n",
      "   -------- ----------------------------- 143.4/626.3 kB 293.9 kB/s eta 0:00:02\n",
      "   ----------- -------------------------- 194.6/626.3 kB 280.5 kB/s eta 0:00:02\n",
      "   ----------- -------------------------- 194.6/626.3 kB 280.5 kB/s eta 0:00:02\n",
      "   ------------ ------------------------- 204.8/626.3 kB 276.8 kB/s eta 0:00:02\n",
      "   ------------- ------------------------ 225.3/626.3 kB 280.7 kB/s eta 0:00:02\n",
      "   --------------- ---------------------- 256.0/626.3 kB 291.2 kB/s eta 0:00:02\n",
      "   ----------------- -------------------- 286.7/626.3 kB 316.0 kB/s eta 0:00:02\n",
      "   ------------------ ------------------- 307.2/626.3 kB 327.4 kB/s eta 0:00:01\n",
      "   ------------------- ------------------ 327.7/626.3 kB 338.4 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 358.4/626.3 kB 342.7 kB/s eta 0:00:01\n",
      "   --------------------- ---------------- 358.4/626.3 kB 342.7 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 409.6/626.3 kB 355.0 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 409.6/626.3 kB 355.0 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 409.6/626.3 kB 355.0 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 409.6/626.3 kB 355.0 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 471.0/626.3 kB 359.4 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 491.5/626.3 kB 362.4 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 491.5/626.3 kB 362.4 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 491.5/626.3 kB 362.4 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 522.2/626.3 kB 337.7 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 604.2/626.3 kB 383.9 kB/s eta 0:00:01\n",
      "   -------------------------------------  614.4/626.3 kB 382.9 kB/s eta 0:00:01\n",
      "   -------------------------------------  614.4/626.3 kB 382.9 kB/s eta 0:00:01\n",
      "   -------------------------------------  614.4/626.3 kB 382.9 kB/s eta 0:00:01\n",
      "   -------------------------------------- 626.3/626.3 kB 361.8 kB/s eta 0:00:00\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.18.0.post0\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1209b09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import cmudict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d084494",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\trupti\n",
      "[nltk_data]     kanade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\trupti\n",
      "[nltk_data]     kanade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\trupti kanade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package cmudict to C:\\Users\\trupti\n",
      "[nltk_data]     kanade\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\cmudict.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('cmudict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a914a978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Downloading python_docx-1.1.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\trupti kanade\\anaconda3\\lib\\site-packages (from python-docx) (4.9.3)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\trupti kanade\\anaconda3\\lib\\site-packages (from python-docx) (4.9.0)\n",
      "Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
      "   ---------------------------------------- 0.0/239.6 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/239.6 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/239.6 kB ? eta -:--:--\n",
      "   ----- --------------------------------- 30.7/239.6 kB 330.3 kB/s eta 0:00:01\n",
      "   ------ -------------------------------- 41.0/239.6 kB 281.8 kB/s eta 0:00:01\n",
      "   ----------- --------------------------- 71.7/239.6 kB 328.6 kB/s eta 0:00:01\n",
      "   ----------------- -------------------- 112.6/239.6 kB 437.6 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 143.4/239.6 kB 473.7 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 153.6/239.6 kB 437.1 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 174.1/239.6 kB 456.4 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 194.6/239.6 kB 421.8 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 204.8/239.6 kB 414.8 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 204.8/239.6 kB 414.8 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 204.8/239.6 kB 414.8 kB/s eta 0:00:01\n",
      "   -------------------------------------- 239.6/239.6 kB 386.1 kB/s eta 0:00:00\n",
      "Installing collected packages: python-docx\n",
      "Successfully installed python-docx-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cba44a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Analysis\n",
      "\n",
      "Objective of this document is to explain methodology adopted to perform text analysis to drive sentimental opinion, sentiment scores, readability, passive words, personal pronouns and etc.\n",
      "\n",
      "Table of Contents\n",
      "\n",
      "Sentimental Analysis\n",
      "Sentimental analysis is the process of determining whether a piece of writing is positive, negative, or neutral. The below Algorithm is designed for use in Financial Texts. It consists of steps:\n",
      "Cleaning using Stop Words Lists\n",
      "The Stop Words Lists (found in the folder StopWords) are used to clean the text so that Sentiment Analysis can be performed by excluding the words found in Stop Words List. \n",
      "Creating a dictionary of Positive and Negative words\n",
      "The Master Dictionary (found in the folder MasterDictionary) is used for creating a dictionary of Positive and Negative words. We add only those words in the dictionary if they are not found in the Stop Words Lists. \n",
      "Extracting Derived variables\n",
      "We convert the text into a list of tokens using the nltk tokenize module and use these tokens to calculate the 4 variables described below:\n",
      "Positive Score: This score is calculated by assigning the value of +1 for each word if found in the Positive Dictionary and then adding up all the values.\n",
      "Negative Score: This score is calculated by assigning the value of -1 for each word if found in the Negative Dictionary and then adding up all the values. We multiply the score with -1 so that the score is a positive number.\n",
      "Polarity Score: This is the score that determines if a given text is positive or negative in nature. It is calculated by using the formula: \n",
      "Polarity Score = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\n",
      "Range is from -1 to +1\n",
      "Subjectivity Score: This is the score that determines if a given text is objective or subjective. It is calculated by using the formula: \n",
      "Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)\n",
      "Range is from 0 to +1\n",
      "\n",
      "Analysis of Readability\n",
      "Analysis of Readability is calculated using the Gunning Fox index formula described below.\n",
      "Average Sentence Length = the number of words / the number of sentences\n",
      "Percentage of Complex words = the number of complex words / the number of words \n",
      "Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
      "\n",
      "Average Number of Words Per Sentence\n",
      "The formula for calculating is:\n",
      "Average Number of Words Per Sentence = the total number of words / the total number of sentences\n",
      "\n",
      "Complex Word Count\n",
      "Complex words are words in the text that contain more than two syllables.\n",
      "\n",
      "Word Count\n",
      "We count the total cleaned words present in the text by \n",
      "removing the stop words (using stopwords class of nltk package).\n",
      "removing any punctuations like ? ! , . from the word before counting.\n",
      "\n",
      "Syllable Count Per Word\n",
      "We count the number of Syllables in each word of the text by counting the vowels present in each word. We also handle some exceptions like words ending with \"es\",\"ed\" by not counting them as a syllable.\n",
      "\n",
      "Personal Pronouns\n",
      "To calculate Personal Pronouns mentioned in the text, we use regex to find the counts of the words - “I,” “we,” “my,” “ours,” and “us”. Special care is taken so that the country name US is not included in the list.\n",
      "\n",
      "Average Word Length\n",
      "Average Word Length is calculated by the formula:\n",
      "Sum of the total number of characters in each word/Total number of words\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "\n",
    "\n",
    "doc = Document('Text Analysis.docx')  # Replace 'your_document.docx' with the path to your .docx file\n",
    "\n",
    "\n",
    "text = ''\n",
    "for paragraph in doc.paragraphs:\n",
    "    text += paragraph.text + '\\n'\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eff3809b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text Analysis\\n\\nObjective of this document is to explain methodology adopted to perform text analysis to drive sentimental opinion, sentiment scores, readability, passive words, personal pronouns and etc.', 'Table of Contents\\n\\nSentimental Analysis\\nSentimental analysis is the process of determining whether a piece of writing is positive, negative, or neutral.', 'The below Algorithm is designed for use in Financial Texts.', 'It consists of steps:\\nCleaning using Stop Words Lists\\nThe Stop Words Lists (found in the folder StopWords) are used to clean the text so that Sentiment Analysis can be performed by excluding the words found in Stop Words List.', 'Creating a dictionary of Positive and Negative words\\nThe Master Dictionary (found in the folder MasterDictionary) is used for creating a dictionary of Positive and Negative words.', 'We add only those words in the dictionary if they are not found in the Stop Words Lists.', 'Extracting Derived variables\\nWe convert the text into a list of tokens using the nltk tokenize module and use these tokens to calculate the 4 variables described below:\\nPositive Score: This score is calculated by assigning the value of +1 for each word if found in the Positive Dictionary and then adding up all the values.', 'Negative Score: This score is calculated by assigning the value of -1 for each word if found in the Negative Dictionary and then adding up all the values.', 'We multiply the score with -1 so that the score is a positive number.', 'Polarity Score: This is the score that determines if a given text is positive or negative in nature.', 'It is calculated by using the formula: \\nPolarity Score = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\\nRange is from -1 to +1\\nSubjectivity Score: This is the score that determines if a given text is objective or subjective.', 'It is calculated by using the formula: \\nSubjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)\\nRange is from 0 to +1\\n\\nAnalysis of Readability\\nAnalysis of Readability is calculated using the Gunning Fox index formula described below.', 'Average Sentence Length = the number of words / the number of sentences\\nPercentage of Complex words = the number of complex words / the number of words \\nFog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\\n\\nAverage Number of Words Per Sentence\\nThe formula for calculating is:\\nAverage Number of Words Per Sentence = the total number of words / the total number of sentences\\n\\nComplex Word Count\\nComplex words are words in the text that contain more than two syllables.', 'Word Count\\nWe count the total cleaned words present in the text by \\nremoving the stop words (using stopwords class of nltk package).', 'removing any punctuations like ?', '!', ', .', 'from the word before counting.', 'Syllable Count Per Word\\nWe count the number of Syllables in each word of the text by counting the vowels present in each word.', 'We also handle some exceptions like words ending with \"es\",\"ed\" by not counting them as a syllable.', 'Personal Pronouns\\nTo calculate Personal Pronouns mentioned in the text, we use regex to find the counts of the words - “I,” “we,” “my,” “ours,” and “us”.', 'Special care is taken so that the country name US is not included in the list.', 'Average Word Length\\nAverage Word Length is calculated by the formula:\\nSum of the total number of characters in each word/Total number of words']\n",
      "['Text', 'Analysis', 'Objective', 'of', 'this', 'document', 'is', 'to', 'explain', 'methodology', 'adopted', 'to', 'perform', 'text', 'analysis', 'to', 'drive', 'sentimental', 'opinion', ',', 'sentiment', 'scores', ',', 'readability', ',', 'passive', 'words', ',', 'personal', 'pronouns', 'and', 'etc', '.', 'Table', 'of', 'Contents', 'Sentimental', 'Analysis', 'Sentimental', 'analysis', 'is', 'the', 'process', 'of', 'determining', 'whether', 'a', 'piece', 'of', 'writing', 'is', 'positive', ',', 'negative', ',', 'or', 'neutral', '.', 'The', 'below', 'Algorithm', 'is', 'designed', 'for', 'use', 'in', 'Financial', 'Texts', '.', 'It', 'consists', 'of', 'steps', ':', 'Cleaning', 'using', 'Stop', 'Words', 'Lists', 'The', 'Stop', 'Words', 'Lists', '(', 'found', 'in', 'the', 'folder', 'StopWords', ')', 'are', 'used', 'to', 'clean', 'the', 'text', 'so', 'that', 'Sentiment', 'Analysis', 'can', 'be', 'performed', 'by', 'excluding', 'the', 'words', 'found', 'in', 'Stop', 'Words', 'List', '.', 'Creating', 'a', 'dictionary', 'of', 'Positive', 'and', 'Negative', 'words', 'The', 'Master', 'Dictionary', '(', 'found', 'in', 'the', 'folder', 'MasterDictionary', ')', 'is', 'used', 'for', 'creating', 'a', 'dictionary', 'of', 'Positive', 'and', 'Negative', 'words', '.', 'We', 'add', 'only', 'those', 'words', 'in', 'the', 'dictionary', 'if', 'they', 'are', 'not', 'found', 'in', 'the', 'Stop', 'Words', 'Lists', '.', 'Extracting', 'Derived', 'variables', 'We', 'convert', 'the', 'text', 'into', 'a', 'list', 'of', 'tokens', 'using', 'the', 'nltk', 'tokenize', 'module', 'and', 'use', 'these', 'tokens', 'to', 'calculate', 'the', '4', 'variables', 'described', 'below', ':', 'Positive', 'Score', ':', 'This', 'score', 'is', 'calculated', 'by', 'assigning', 'the', 'value', 'of', '+1', 'for', 'each', 'word', 'if', 'found', 'in', 'the', 'Positive', 'Dictionary', 'and', 'then', 'adding', 'up', 'all', 'the', 'values', '.', 'Negative', 'Score', ':', 'This', 'score', 'is', 'calculated', 'by', 'assigning', 'the', 'value', 'of', '-1', 'for', 'each', 'word', 'if', 'found', 'in', 'the', 'Negative', 'Dictionary', 'and', 'then', 'adding', 'up', 'all', 'the', 'values', '.', 'We', 'multiply', 'the', 'score', 'with', '-1', 'so', 'that', 'the', 'score', 'is', 'a', 'positive', 'number', '.', 'Polarity', 'Score', ':', 'This', 'is', 'the', 'score', 'that', 'determines', 'if', 'a', 'given', 'text', 'is', 'positive', 'or', 'negative', 'in', 'nature', '.', 'It', 'is', 'calculated', 'by', 'using', 'the', 'formula', ':', 'Polarity', 'Score', '=', '(', 'Positive', 'Score', '–', 'Negative', 'Score', ')', '/', '(', '(', 'Positive', 'Score', '+', 'Negative', 'Score', ')', '+', '0.000001', ')', 'Range', 'is', 'from', '-1', 'to', '+1', 'Subjectivity', 'Score', ':', 'This', 'is', 'the', 'score', 'that', 'determines', 'if', 'a', 'given', 'text', 'is', 'objective', 'or', 'subjective', '.', 'It', 'is', 'calculated', 'by', 'using', 'the', 'formula', ':', 'Subjectivity', 'Score', '=', '(', 'Positive', 'Score', '+', 'Negative', 'Score', ')', '/', '(', '(', 'Total', 'Words', 'after', 'cleaning', ')', '+', '0.000001', ')', 'Range', 'is', 'from', '0', 'to', '+1', 'Analysis', 'of', 'Readability', 'Analysis', 'of', 'Readability', 'is', 'calculated', 'using', 'the', 'Gunning', 'Fox', 'index', 'formula', 'described', 'below', '.', 'Average', 'Sentence', 'Length', '=', 'the', 'number', 'of', 'words', '/', 'the', 'number', 'of', 'sentences', 'Percentage', 'of', 'Complex', 'words', '=', 'the', 'number', 'of', 'complex', 'words', '/', 'the', 'number', 'of', 'words', 'Fog', 'Index', '=', '0.4', '*', '(', 'Average', 'Sentence', 'Length', '+', 'Percentage', 'of', 'Complex', 'words', ')', 'Average', 'Number', 'of', 'Words', 'Per', 'Sentence', 'The', 'formula', 'for', 'calculating', 'is', ':', 'Average', 'Number', 'of', 'Words', 'Per', 'Sentence', '=', 'the', 'total', 'number', 'of', 'words', '/', 'the', 'total', 'number', 'of', 'sentences', 'Complex', 'Word', 'Count', 'Complex', 'words', 'are', 'words', 'in', 'the', 'text', 'that', 'contain', 'more', 'than', 'two', 'syllables', '.', 'Word', 'Count', 'We', 'count', 'the', 'total', 'cleaned', 'words', 'present', 'in', 'the', 'text', 'by', 'removing', 'the', 'stop', 'words', '(', 'using', 'stopwords', 'class', 'of', 'nltk', 'package', ')', '.', 'removing', 'any', 'punctuations', 'like', '?', '!', ',', '.', 'from', 'the', 'word', 'before', 'counting', '.', 'Syllable', 'Count', 'Per', 'Word', 'We', 'count', 'the', 'number', 'of', 'Syllables', 'in', 'each', 'word', 'of', 'the', 'text', 'by', 'counting', 'the', 'vowels', 'present', 'in', 'each', 'word', '.', 'We', 'also', 'handle', 'some', 'exceptions', 'like', 'words', 'ending', 'with', '``', 'es', \"''\", ',', \"''\", 'ed', \"''\", 'by', 'not', 'counting', 'them', 'as', 'a', 'syllable', '.', 'Personal', 'Pronouns', 'To', 'calculate', 'Personal', 'Pronouns', 'mentioned', 'in', 'the', 'text', ',', 'we', 'use', 'regex', 'to', 'find', 'the', 'counts', 'of', 'the', 'words', '-', '“', 'I', ',', '”', '“', 'we', ',', '”', '“', 'my', ',', '”', '“', 'ours', ',', '”', 'and', '“', 'us', '”', '.', 'Special', 'care', 'is', 'taken', 'so', 'that', 'the', 'country', 'name', 'US', 'is', 'not', 'included', 'in', 'the', 'list', '.', 'Average', 'Word', 'Length', 'Average', 'Word', 'Length', 'is', 'calculated', 'by', 'the', 'formula', ':', 'Sum', 'of', 'the', 'total', 'number', 'of', 'characters', 'in', 'each', 'word/Total', 'number', 'of', 'words']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text into sentences and words\n",
    "sentences = sent_tokenize(text)\n",
    "words = word_tokenize(text)\n",
    "print(sentences)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d9e3a3",
   "metadata": {},
   "source": [
    "# Step 5 Output Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4613a815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3401"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate word count\n",
    "word_count = len(text)\n",
    "word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03915f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2810\n",
      "0.8262275801234931\n"
     ]
    }
   ],
   "source": [
    "# Calculated average word length\n",
    "total_word_length = sum(len(word) for word in words)\n",
    "average_word_length = total_word_length / word_count\n",
    "print(total_word_length)\n",
    "print(average_word_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "daee17a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculated POSITIVE SCORE, NEGATIVE SCORE, POLARITY SCORE, SUBJECTIVITY SCORE using TextBlob\n",
    "blob = TextBlob(text)\n",
    "positive_score = sum(1 for sentence in blob.sentences if sentence.sentiment.polarity > 0)\n",
    "negative_score = sum(1 for sentence in blob.sentences if sentence.sentiment.polarity < 0)\n",
    "polarity_score = blob.sentiment.polarity\n",
    "subjectivity_score = blob.sentiment.subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32e4747b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.130434782608695"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculated average sentence length\n",
    "avg_sentence_length = sum(len(sent.split()) for sent in sentences) / len(sentences)\n",
    "avg_sentence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1e119bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.999117906498089"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculated percentage of complex words \n",
    "d = cmudict.dict()\n",
    "num_complex_words = 0\n",
    "for word in words:\n",
    "    \n",
    "    if word.lower() in d:\n",
    "        # Get the list of syllables for the word\n",
    "        syllables_list = d[word.lower()]\n",
    "       \n",
    "        syllables = [len(list(y for y in x if y[-1].isdigit())) for x in syllables_list if len(x) > 0]\n",
    "       \n",
    "        if max(syllables, default=0) > 2:\n",
    "            num_complex_words += 1\n",
    "\n",
    "# Calculated percentage of complex words\n",
    "percentage_complex_words = (num_complex_words / word_count) * 100\n",
    "percentage_complex_words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5569b61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.251821075642715"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculated FOG INDEX\n",
    "fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "fog_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "00c20b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147.8695652173913"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculated average number of words per sentence\n",
    "avg_words_per_sentence = word_count / len(sentences)\n",
    "avg_words_per_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf66b1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40017641870038223"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculated syllable per word\n",
    "def syllable_count(word):\n",
    "   \n",
    "    if word.lower() in d:\n",
    "       \n",
    "        syllables_list = d[word.lower()]\n",
    "        \n",
    "        syllables = [len(list(y for y in x if y[-1].isdigit())) for x in syllables_list if len(x) > 0]\n",
    "       \n",
    "        return sum(syllables)\n",
    "    else:\n",
    "       \n",
    "        return 0\n",
    "\n",
    "# Calculated syllable per word for each word in the text\n",
    "syllable_counts = [syllable_count(word) for word in words]\n",
    "syllable_counts\n",
    "# Calculated the total syllable count\n",
    "total_syllable_count = sum(syllable_counts)\n",
    "total_syllable_count\n",
    "# Calculated syllable per word\n",
    "syllable_per_word = total_syllable_count / word_count\n",
    "syllable_per_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d4f7754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculated personal pronouns count\n",
    "tagged_words = pos_tag(words)\n",
    "personal_pronouns = sum(1 for word, pos in tagged_words if pos == 'PRP')\n",
    "personal_pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a7c111aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output  of the results\n",
    "results = {\n",
    "    \"POSITIVE SCORE\": positive_score,\n",
    "    \"NEGATIVE SCORE\": negative_score,\n",
    "    \"POLARITY SCORE\": polarity_score,\n",
    "    \"SUBJECTIVITY SCORE\": subjectivity_score,\n",
    "    \"AVG SENTENCE LENGTH\": avg_sentence_length,\n",
    "    \"PERCENTAGE OF COMPLEX WORDS\": percentage_complex_words,\n",
    "    \"FOG INDEX\": fog_index,\n",
    "    \"AVG NUMBER OF WORDS PER SENTENCE\": avg_words_per_sentence,\n",
    "    \"COMPLEX WORD COUNT\": complex_word_count,\n",
    "    \"WORD COUNT\": word_count,\n",
    "    \"SYLLABLE PER WORD\": syllable_per_word,\n",
    "    \"PERSONAL PRONOUNS\": personal_pronouns,\n",
    "    \"AVG WORD LENGTH\": average_word_length\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "37d59d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame(results.items(), columns=[\"Variable\", \"Value\"])\n",
    "output_df.to_excel(\"text_analysis_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "026e7673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blackassign0001</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blackassign0002</td>\n",
       "      <td>https://insights.blackcoffer.com/rising-it-cit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blackassign0003</td>\n",
       "      <td>https://insights.blackcoffer.com/internet-dema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blackassign0004</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-cyber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blackassign0005</td>\n",
       "      <td>https://insights.blackcoffer.com/ott-platform-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>blackassign0096</td>\n",
       "      <td>https://insights.blackcoffer.com/what-is-the-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>blackassign0097</td>\n",
       "      <td>https://insights.blackcoffer.com/impact-of-cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>blackassign0098</td>\n",
       "      <td>https://insights.blackcoffer.com/contribution-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>blackassign0099</td>\n",
       "      <td>https://insights.blackcoffer.com/how-covid-19-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>blackassign0100</td>\n",
       "      <td>https://insights.blackcoffer.com/how-will-covi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             URL_ID                                                URL\n",
       "0   blackassign0001  https://insights.blackcoffer.com/rising-it-cit...\n",
       "1   blackassign0002  https://insights.blackcoffer.com/rising-it-cit...\n",
       "2   blackassign0003  https://insights.blackcoffer.com/internet-dema...\n",
       "3   blackassign0004  https://insights.blackcoffer.com/rise-of-cyber...\n",
       "4   blackassign0005  https://insights.blackcoffer.com/ott-platform-...\n",
       "..              ...                                                ...\n",
       "95  blackassign0096  https://insights.blackcoffer.com/what-is-the-r...\n",
       "96  blackassign0097  https://insights.blackcoffer.com/impact-of-cov...\n",
       "97  blackassign0098  https://insights.blackcoffer.com/contribution-...\n",
       "98  blackassign0099  https://insights.blackcoffer.com/how-covid-19-...\n",
       "99  blackassign0100  https://insights.blackcoffer.com/how-will-covi...\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the input data from \"Input.xlsx\"\n",
    "input_data = pd.read_excel(\"Input.xlsx\")\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ba6b4fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['URL_ID', 'URL']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_variables = input_data.columns.tolist()\n",
    "input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "718a56d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'URL_ID': None, 'URL': None}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = dict.fromkeys(input_variables)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "40226487",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.update({\n",
    "    \"POSITIVE SCORE\": positive_score,\n",
    "    \"NEGATIVE SCORE\": negative_score,\n",
    "    \"POLARITY SCORE\": polarity_score,\n",
    "    \"SUBJECTIVITY SCORE\": subjectivity_score,\n",
    "    \"AVG SENTENCE LENGTH\": avg_sentence_length,\n",
    "    \"PERCENTAGE OF COMPLEX WORDS\": percentage_complex_words,\n",
    "    \"FOG INDEX\": fog_index,\n",
    "    \"AVG NUMBER OF WORDS PER SENTENCE\": avg_words_per_sentence,\n",
    "    \"COMPLEX WORD COUNT\": complex_word_count,\n",
    "    \"WORD COUNT\": word_count,\n",
    "    \"SYLLABLE PER WORD\": syllable_per_word,\n",
    "    \"PERSONAL PRONOUNS\": personal_pronouns,\n",
    "    \"AVG WORD LENGTH\": average_word_length\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aedc1d",
   "metadata": {},
   "source": [
    "# Explaination-->Final Output \n",
    "### 1) Combined the input variables with the computed results.\n",
    "### 2) Created a DataFrame from the results dictionary and saved it to an Excel file.\n",
    "### 3) Generated the final output file containing all the input variables and computed results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75b584f",
   "metadata": {},
   "source": [
    "# This is the Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8d42c043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>URL_ID</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>URL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POSITIVE SCORE</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEGATIVE SCORE</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POLARITY SCORE</td>\n",
       "      <td>-0.072707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SUBJECTIVITY SCORE</td>\n",
       "      <td>0.492000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AVG SENTENCE LENGTH</td>\n",
       "      <td>25.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PERCENTAGE OF COMPLEX WORDS</td>\n",
       "      <td>2.999118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FOG INDEX</td>\n",
       "      <td>11.251821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AVG NUMBER OF WORDS PER SENTENCE</td>\n",
       "      <td>147.869565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>COMPLEX WORD COUNT</td>\n",
       "      <td>318.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>WORD COUNT</td>\n",
       "      <td>3401.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SYLLABLE PER WORD</td>\n",
       "      <td>0.400176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PERSONAL PRONOUNS</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AVG WORD LENGTH</td>\n",
       "      <td>0.826228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Variable        Value\n",
       "0                             URL_ID          NaN\n",
       "1                                URL          NaN\n",
       "2                     POSITIVE SCORE     4.000000\n",
       "3                     NEGATIVE SCORE     9.000000\n",
       "4                     POLARITY SCORE    -0.072707\n",
       "5                 SUBJECTIVITY SCORE     0.492000\n",
       "6                AVG SENTENCE LENGTH    25.130435\n",
       "7        PERCENTAGE OF COMPLEX WORDS     2.999118\n",
       "8                          FOG INDEX    11.251821\n",
       "9   AVG NUMBER OF WORDS PER SENTENCE   147.869565\n",
       "10                COMPLEX WORD COUNT   318.000000\n",
       "11                        WORD COUNT  3401.000000\n",
       "12                 SYLLABLE PER WORD     0.400176\n",
       "13                 PERSONAL PRONOUNS    16.000000\n",
       "14                   AVG WORD LENGTH     0.826228"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Created a DataFrame from the results dictionary\n",
    "output_df = pd.DataFrame(results.items(), columns=[\"Variable\", \"Value\"])\n",
    "output_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1552d01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to \"text_analysis_results.xlsx\"\n",
    "output_df.to_excel(\"text_analysis_results.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "110b6ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Variable</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>URL_ID</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>URL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POSITIVE SCORE</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NEGATIVE SCORE</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POLARITY SCORE</td>\n",
       "      <td>-0.072707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SUBJECTIVITY SCORE</td>\n",
       "      <td>0.492000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AVG SENTENCE LENGTH</td>\n",
       "      <td>25.130435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PERCENTAGE OF COMPLEX WORDS</td>\n",
       "      <td>2.999118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FOG INDEX</td>\n",
       "      <td>11.251821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AVG NUMBER OF WORDS PER SENTENCE</td>\n",
       "      <td>147.869565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>COMPLEX WORD COUNT</td>\n",
       "      <td>318.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>WORD COUNT</td>\n",
       "      <td>3401.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>SYLLABLE PER WORD</td>\n",
       "      <td>0.400176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PERSONAL PRONOUNS</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>AVG WORD LENGTH</td>\n",
       "      <td>0.826228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Variable        Value\n",
       "0                             URL_ID          NaN\n",
       "1                                URL          NaN\n",
       "2                     POSITIVE SCORE     4.000000\n",
       "3                     NEGATIVE SCORE     9.000000\n",
       "4                     POLARITY SCORE    -0.072707\n",
       "5                 SUBJECTIVITY SCORE     0.492000\n",
       "6                AVG SENTENCE LENGTH    25.130435\n",
       "7        PERCENTAGE OF COMPLEX WORDS     2.999118\n",
       "8                          FOG INDEX    11.251821\n",
       "9   AVG NUMBER OF WORDS PER SENTENCE   147.869565\n",
       "10                COMPLEX WORD COUNT   318.000000\n",
       "11                        WORD COUNT  3401.000000\n",
       "12                 SYLLABLE PER WORD     0.400176\n",
       "13                 PERSONAL PRONOUNS    16.000000\n",
       "14                   AVG WORD LENGTH     0.826228"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op = pd.read_excel('text_analysis_results.xlsx')\n",
    "op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee198292",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
